{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPGW02 Web Economics - Coursework\n",
    "# Online Advertising: Optimal Bidding Strategy\n",
    "### Maximilian Bartolo\n",
    "Date: 07 April 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This work concerns an online advertising problem. The objective is to help advertisers to form a bidding strategy in order to place their ads online in a real-time bidding system. The aim is to generate a model-driven algorithm capable of bidding in real-time based display advertising.\n",
    "\n",
    "The performance of the model will be (mainly) evaluated on the Click-through Rate (CTR) achieved on the test set. Conversions (number of clicked bids) is also a significant factor.\n",
    "\n",
    "Other contributing factors include:\n",
    "* CVR (Conversion Rates)\n",
    "* Spend (Total Money Paid)\n",
    "* Average CPM (Cost per Mille)\n",
    "* Average CPC (Cost per Click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics\n",
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is an extract from the iPinYou demand side platform (DSP) real-time bidding dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Python Libraries\n",
    "Let's start off my importing the libraries and packages we'll be using for our analysis as well as setting our default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import glob, re, random, itertools, time\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Importing additional required libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC, OneClassSVM\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set options\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Set a random seed for repeatability\n",
    "rand_seed = 27\n",
    "random.seed(rand_seed)\n",
    "np.random.seed(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data with Pandas\n",
    "The dataset contains *train*, *validation* and *test* data files which all need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "data_path_in = 'data/'\n",
    "df_train = pd.read_csv(data_path_in + \"train.csv\")\n",
    "df_val = pd.read_csv(data_path_in + \"validation.csv\")\n",
    "df_test = pd.read_csv(data_path_in + \"test.csv\")\n",
    "\n",
    "# Remove error data (as instructed)\n",
    "df_train = df_train[df_train.bidprice >= df_train.payprice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>bidid</th>\n",
       "      <th>logtype</th>\n",
       "      <th>userid</th>\n",
       "      <th>useragent</th>\n",
       "      <th>IP</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>slotheight</th>\n",
       "      <th>slotvisibility</th>\n",
       "      <th>slotformat</th>\n",
       "      <th>slotprice</th>\n",
       "      <th>creative</th>\n",
       "      <th>bidprice</th>\n",
       "      <th>payprice</th>\n",
       "      <th>keypage</th>\n",
       "      <th>advertiser</th>\n",
       "      <th>usertag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>fdfae6789b787899f1b875de3ab8b21a</td>\n",
       "      <td>1</td>\n",
       "      <td>u_Vh1OPkFv3q5CFdR</td>\n",
       "      <td>windows_ie</td>\n",
       "      <td>180.107.112.*</td>\n",
       "      <td>80</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>612599432d200b093719dd1f372f7a30</td>\n",
       "      <td>300</td>\n",
       "      <td>54</td>\n",
       "      <td>bebefa5efe83beee17a3d245e7c5085b</td>\n",
       "      <td>1458</td>\n",
       "      <td>13866,10063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>372883147cdefbc495fb5082f79dde9b</td>\n",
       "      <td>1</td>\n",
       "      <td>u_VhkRL6dCOTLsq-c</td>\n",
       "      <td>windows_chrome</td>\n",
       "      <td>125.120.199.*</td>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8dff45ed862a740986dbe688aafee7e5</td>\n",
       "      <td>249</td>\n",
       "      <td>42</td>\n",
       "      <td>15b749127478946f161a54dc0dad27c8</td>\n",
       "      <td>3476</td>\n",
       "      <td>10063,10111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   click  weekday  hour                             bidid  logtype  \\\n",
       "0      0        1    14  fdfae6789b787899f1b875de3ab8b21a        1   \n",
       "1      0        2    20  372883147cdefbc495fb5082f79dde9b        1   \n",
       "\n",
       "              userid       useragent             IP  region  city  \\\n",
       "0  u_Vh1OPkFv3q5CFdR      windows_ie  180.107.112.*      80    85   \n",
       "1  u_VhkRL6dCOTLsq-c  windows_chrome  125.120.199.*      94    95   \n",
       "\n",
       "      ...      slotheight slotvisibility slotformat slotprice  \\\n",
       "0     ...              60              1          0         5   \n",
       "1     ...             250              1          0         5   \n",
       "\n",
       "                           creative  bidprice  payprice  \\\n",
       "0  612599432d200b093719dd1f372f7a30       300        54   \n",
       "1  8dff45ed862a740986dbe688aafee7e5       249        42   \n",
       "\n",
       "                            keypage advertiser      usertag  \n",
       "0  bebefa5efe83beee17a3d245e7c5085b       1458  13866,10063  \n",
       "1  15b749127478946f161a54dc0dad27c8       3476  10063,10111  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspecting the data\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "\n",
    "By analyzing and understanding the structure and content of the available data, we can determine which variables would add value to our model and business case. We therefore drop any columns which cannot be used for our prediction model or which provide no added value.\n",
    "\n",
    "We remove the following columns for the reasons below:\n",
    "* logtype - all values are 1\n",
    "* userid - 2558996 unique out of 2664159\n",
    "* urlid - all values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the dataset\n",
    "cols_to_remove = ['logtype', 'userid', 'urlid']\n",
    "cols_of_interest = [col for col in list(df_train.columns) if col not in cols_to_remove]\n",
    "df_train = df_train[cols_of_interest]\n",
    "df_val = df_val[cols_of_interest]\n",
    "\n",
    "# For the test set, there are less columns than the other two datasets\n",
    "cols_of_interest = [col for col in list(df_test.columns) if col not in cols_to_remove]\n",
    "df_test = df_test[cols_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2664159 entries, 0 to 2697737\n",
      "Data columns (total 23 columns):\n",
      "click             2664159 non-null int64\n",
      "weekday           2664159 non-null int64\n",
      "hour              2664159 non-null int64\n",
      "bidid             2664159 non-null object\n",
      "useragent         2664159 non-null object\n",
      "IP                2664159 non-null object\n",
      "region            2664159 non-null int64\n",
      "city              2664159 non-null int64\n",
      "adexchange        2664159 non-null object\n",
      "domain            2664159 non-null object\n",
      "url               2664159 non-null object\n",
      "slotid            2664159 non-null object\n",
      "slotwidth         2664159 non-null int64\n",
      "slotheight        2664159 non-null int64\n",
      "slotvisibility    2664159 non-null object\n",
      "slotformat        2664159 non-null object\n",
      "slotprice         2664159 non-null int64\n",
      "creative          2664159 non-null object\n",
      "bidprice          2664159 non-null int64\n",
      "payprice          2664159 non-null int64\n",
      "keypage           2664159 non-null object\n",
      "advertiser        2664159 non-null int64\n",
      "usertag           2664159 non-null object\n",
      "dtypes: int64(11), object(12)\n",
      "memory usage: 487.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Get more information about the dataframe\n",
    "df_train.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>slotwidth</th>\n",
       "      <th>slotheight</th>\n",
       "      <th>slotprice</th>\n",
       "      <th>bidprice</th>\n",
       "      <th>payprice</th>\n",
       "      <th>advertiser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "      <td>2.664159e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.454510e-04</td>\n",
       "      <td>2.889097e+00</td>\n",
       "      <td>1.349015e+01</td>\n",
       "      <td>1.495184e+02</td>\n",
       "      <td>1.544642e+02</td>\n",
       "      <td>5.774678e+02</td>\n",
       "      <td>1.884807e+02</td>\n",
       "      <td>2.673956e+01</td>\n",
       "      <td>2.729818e+02</td>\n",
       "      <td>7.814598e+01</td>\n",
       "      <td>2.840225e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.729278e-02</td>\n",
       "      <td>2.055271e+00</td>\n",
       "      <td>6.568752e+00</td>\n",
       "      <td>1.020634e+02</td>\n",
       "      <td>1.033031e+02</td>\n",
       "      <td>3.216016e+02</td>\n",
       "      <td>1.312695e+02</td>\n",
       "      <td>3.706526e+01</td>\n",
       "      <td>2.890392e+01</td>\n",
       "      <td>5.984167e+01</td>\n",
       "      <td>7.845256e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.200000e+02</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.270000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.458000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>7.900000e+01</td>\n",
       "      <td>7.900000e+01</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.410000e+02</td>\n",
       "      <td>3.300000e+01</td>\n",
       "      <td>2.259000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>1.460000e+02</td>\n",
       "      <td>1.520000e+02</td>\n",
       "      <td>3.360000e+02</td>\n",
       "      <td>9.000000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.940000e+02</td>\n",
       "      <td>6.800000e+01</td>\n",
       "      <td>3.358000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>2.160000e+02</td>\n",
       "      <td>2.220000e+02</td>\n",
       "      <td>9.600000e+02</td>\n",
       "      <td>2.500000e+02</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>3.427000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>3.950000e+02</td>\n",
       "      <td>3.990000e+02</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>6.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>3.476000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              click       weekday          hour        region          city  \\\n",
       "count  2.664159e+06  2.664159e+06  2.664159e+06  2.664159e+06  2.664159e+06   \n",
       "mean   7.454510e-04  2.889097e+00  1.349015e+01  1.495184e+02  1.544642e+02   \n",
       "std    2.729278e-02  2.055271e+00  6.568752e+00  1.020634e+02  1.033031e+02   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  1.000000e+00  1.000000e+01  7.900000e+01  7.900000e+01   \n",
       "50%    0.000000e+00  3.000000e+00  1.400000e+01  1.460000e+02  1.520000e+02   \n",
       "75%    0.000000e+00  5.000000e+00  1.900000e+01  2.160000e+02  2.220000e+02   \n",
       "max    1.000000e+00  6.000000e+00  2.300000e+01  3.950000e+02  3.990000e+02   \n",
       "\n",
       "          slotwidth    slotheight     slotprice      bidprice      payprice  \\\n",
       "count  2.664159e+06  2.664159e+06  2.664159e+06  2.664159e+06  2.664159e+06   \n",
       "mean   5.774678e+02  1.884807e+02  2.673956e+01  2.729818e+02  7.814598e+01   \n",
       "std    3.216016e+02  1.312695e+02  3.706526e+01  2.890392e+01  5.984167e+01   \n",
       "min    1.200000e+02  5.000000e+01  0.000000e+00  2.270000e+02  0.000000e+00   \n",
       "25%    3.000000e+02  9.000000e+01  0.000000e+00  2.410000e+02  3.300000e+01   \n",
       "50%    3.360000e+02  9.000000e+01  5.000000e+00  2.940000e+02  6.800000e+01   \n",
       "75%    9.600000e+02  2.500000e+02  5.000000e+01  3.000000e+02  9.900000e+01   \n",
       "max    1.000000e+03  6.000000e+02  3.000000e+02  3.000000e+02  3.000000e+02   \n",
       "\n",
       "         advertiser  \n",
       "count  2.664159e+06  \n",
       "mean   2.840225e+03  \n",
       "std    7.845256e+02  \n",
       "min    1.458000e+03  \n",
       "25%    2.259000e+03  \n",
       "50%    3.358000e+03  \n",
       "75%    3.427000e+03  \n",
       "max    3.476000e+03  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get descriptive statistics about the dataframe\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary statistics, it could be observed that none of the training data has missing values, however, on further observation we notice that missing values do exist in the data and are stored as string representations such as *null* or *na*, therefore, we replace all string representations with actuall *NULL* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace all \"null\" occurrences with np.nan\n",
    "list_replace_from = ['null', 'Na', 'nan']\n",
    "list_replace_to = [np.nan] * len(list_replace_from)\n",
    "\n",
    "df_train = df_train.replace(list_replace_from, list_replace_to)\n",
    "df_val = df_val.replace(list_replace_from, list_replace_to)\n",
    "df_test = df_test.replace(list_replace_from, list_replace_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the *bidprice*, *payprice* and *click* features which are not present in the *test* dataset do not have missing values, we can impute on the remaining columns. We choose to replace missing values with the feature median and the most common value for non-numeric data types. We could do more advanced imputation such as predicting missing values through regression techniques, however, given the small amount of actual missing values in the dataset, this was deemed unnecessary at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "for col in df_test.columns.values:\n",
    "    # Using whole data to form an exhaustive list of values from which to extract median\n",
    "    data = df_train[col].append(df_val[col]).append(df_test[col])\n",
    "    \n",
    "    if df_test[col].dtypes == 'object':\n",
    "        median_val = data.value_counts().index[0]\n",
    "    else:\n",
    "        median_val = data.median()\n",
    "        \n",
    "    # apply the imputation\n",
    "    df_train[col].fillna(median_val, inplace=True)\n",
    "    df_val[col].fillna(median_val, inplace=True)\n",
    "    df_test[col].fillna(median_val, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data structure, we can extract or manipulate features which could provide added value to our prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price Bucketing\n",
    "We start by creating 5 price bands and allocating every data sample into one of these bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_buckets(price):\n",
    "    if price == 0:\n",
    "        return [1,0,0,0,0]\n",
    "    elif price > 0 and price <= 10:\n",
    "        return [0,1,0,0,0]\n",
    "    elif price > 10 and price <= 50:\n",
    "        return [0,0,1,0,0]\n",
    "    elif price > 50 and price <=100:\n",
    "        return [0,0,0,1,0]\n",
    "    elif price > 100:\n",
    "        return [0,0,0,0,1]\n",
    "    \n",
    "df_train[\"slotprice_0\"],df_train[\"slotprice_1_10\"],df_train[\"slotprice_11_50\"],df_train[\"slotprice_50_100\"],df_train[\"slotprice_50\"] = zip(*df_train.slotprice.map(get_buckets))\n",
    "df_val[\"slotprice_0\"],df_val[\"slotprice_1_10\"],df_val[\"slotprice_11_50\"],df_val[\"slotprice_50_100\"],df_val[\"slotprice_50\"] = zip(*df_val.slotprice.map(get_buckets))\n",
    "df_test[\"slotprice_0\"],df_test[\"slotprice_1_10\"],df_test[\"slotprice_11_50\"],df_test[\"slotprice_50_100\"],df_test[\"slotprice_50\"] = zip(*df_test.slotprice.map(get_buckets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Browser and OS separation\n",
    "We separate browser and operating system into two separate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train[\"os\"],df_train[\"browser\"] = zip(*df_train.useragent.map(lambda x: x.split(\"_\")))\n",
    "df_val[\"os\"],df_val[\"browser\"] = zip(*df_val.useragent.map(lambda x: x.split(\"_\")))\n",
    "df_test[\"os\"],df_test[\"browser\"] = zip(*df_test.useragent.map(lambda x: x.split(\"_\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot Area\n",
    "We define the slot area as the slot width multiplied by the slot height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train[\"slotarea\"] = df_train[\"slotheight\"]*df_train[\"slotwidth\"]\n",
    "df_val[\"slotarea\"] = df_val[\"slotheight\"]*df_val[\"slotwidth\"]\n",
    "df_test[\"slotarea\"] = df_test[\"slotheight\"]*df_test[\"slotwidth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding of Usertags\n",
    "Usertags are currently stored as a comma separated string which cannot be accessed as separate features by our model, only as a combination of usertags. Therefore, to provide the model with greater flexibility we perform one-hot encoding of the separated usertags, essentially creating a new feature for every usertag. The sparsity of the new data structure doesn't hinder model performance too much but provides significantly improved ability for feature manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by identifying all the unique usertags in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df_train.append(df_val).append(df_test)\n",
    "import itertools\n",
    "list_usertags = [x.split(',') for x in data.usertag.values if x is not np.nan]\n",
    "joined_usertags = list(itertools.chain.from_iterable(list_usertags))\n",
    "unique_usertags = sorted(list(set(joined_usertags)))\n",
    "\n",
    "# Results below to speed up processing\n",
    "# unique_usertags = ['10006', '10024', '10031', '10048', '10052', '10057', '10059', '10063', '10067', '10074', '10075', '10076', '10077', '10079', '10083', '10093', '10102', '10110', '10111', '10114', '10115', '10116', '10117', '10118', '10120', '10123', '10125', '10126', '10127', '10129', '10130', '10131', '10133', '10138', '10140', '10142', '10145', '10146', '10147', '10148', '10149', '10684', '11092', '11278', '11379', '11423', '11512', '11576', '11632', '11680', '11724', '11944', '13042', '13403', '13496', '13678', '13776', '13800', '13866', '13874', '14273', '15398', '16593', '16617', '16661', '16706', '16751', '16753']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a new column for every usertag and initialize that to the default value of 0, indicating that particular usertag is not present in the *usertag* string list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add the usertags as columns in the dataframes\n",
    "for usertag_id in unique_usertags: # Add a column for each unique usertag\n",
    "    df_train[\"usertag_\" + usertag_id] = int(0)\n",
    "    df_val[\"usertag_\" + usertag_id] = int(0)\n",
    "    df_test[\"usertag_\" + usertag_id] = int(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we populate the usertags for each of the three datasets using a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Populate the usertags for each dataframe\n",
    "for usertag_id in unique_usertags:\n",
    "    df_train[\"usertag_\" + usertag_id] = df_train.apply(lambda x: 1 if ((x[\"usertag\"] is not np.nan) and \\\n",
    "                                                                       (usertag_id in x[\"usertag\"].split(','))) else 0, axis=1)\n",
    "    df_val[\"usertag_\" + usertag_id] = df_val.apply(lambda x: 1 if ((x[\"usertag\"] is not np.nan) and \\\n",
    "                                                                   (usertag_id in x[\"usertag\"].split(','))) else 0, axis=1)\n",
    "    df_test[\"usertag_\" + usertag_id] = df_test.apply(lambda x: 1 if ((x[\"usertag\"] is not np.nan) and \\\n",
    "                                                                     (usertag_id in x[\"usertag\"].split(','))) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we save and store our cleaned and feature engineered datasets in order to clear up memory and speed up processing for our Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the data\n",
    "data_path_out = 'data/'\n",
    "df_train.to_csv(data_path_out + \"train_clean.csv\", encoding=\"utf-8\", index=False)\n",
    "df_val.to_csv(data_path_out + \"validation_clean.csv\", encoding=\"utf-8\", index=False)\n",
    "df_test.to_csv(data_path_out + \"test_clean.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dataset prepared, we will begin exploratory analysis. This iterative process will help us in building our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we import the cleaned and pre-processed data\n",
    "data_path_in = 'data/'\n",
    "df_train = pd.read_csv(data_path_in + \"train_preprocessed.csv\")\n",
    "df_val = pd.read_csv(data_path_in + \"validation_preprocessed.csv\")\n",
    "df_test = pd.read_csv(data_path_in + \"test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#USERTAGS: HISTOGRAM, CORRELATION CHART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(18,6))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "#Box Plot of Loan Amount Requested\n",
    "df['loan_amnt'].plot.box()\n",
    "plt.title('Box Plot of Loan Amount Requested')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "#Box Plot of Loan Amount Funded\n",
    "df['funded_amnt'].plot.box()\n",
    "plt.title('Box Plot of Loan Amount Funded')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large amount of data and computational expense of training, optimizing and cross-validating such large models, we will take a randomly selected subset of the data to identify the best model and hyper-parameters. We analyze different subset volumes to see at which point we can safely sub-sample without compromising the model. We find that this is at a point where the subsampled training data contains all the outliers (clicks) and approximately 15,000 inliers (non-clicks), significantly undersampling to raise the anomaly ratio.\n",
    "\n",
    "We are opting for a One-Class SVM model which is specifically designed to detect anomalies. From initial testing we notice that our data is not linearly separable and this, a polynomial kernel, while being significantly faster, is of little use. Therefore, we opt for the more complex and significantly computationally slower RBF (Gaussian Radial Base Function) Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Encoding\n",
    "We must first encode all our features from strings or objects to integer values. We could also One-Hot Encode such features to remove ordinality within the data and achieve orthogonality in the vector space, however, this comes at the expense of increasing the number of features, which would have a significant impact on our computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by excluding the usertag features as we know that these are already encoded and scaled as well as the features we cannot use for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_process = df_test.columns.values\n",
    "features_to_process = [x for x in features_to_process if \"usertag_\" not in x]\n",
    "\n",
    "features_to_exclude = ['click', 'bidid', 'bidprice', 'payprice']\n",
    "features_to_process = [x for x in features_to_process if x not in features_to_exclude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the encoding and store all the encoders in case we want to decode at a later point. We have to ensure that our encoder is fit on the entire combination of the three datasets so that it knows how to encode every value in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Label encode and scale all features\n",
    "le = {}\n",
    "\n",
    "for col in features_to_process:\n",
    "    # Encoding only categorical variables\n",
    "    if df_test[col].dtypes == 'object':\n",
    "        # Using whole data to form an exhaustive list of levels\n",
    "        data = df_train[col].append(df_val[col]).append(df_test[col])\n",
    "        \n",
    "        le[col] = preprocessing.LabelEncoder() #define and store a label encoder for every column so we can inverse_transform\n",
    "        le[col].fit(data.values)\n",
    "        \n",
    "        df_train[col] = le[col].transform(df_train[col])\n",
    "        df_val[col] = le[col].transform(df_val[col])        \n",
    "        df_test[col] = le[col].transform(df_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "There are various methods for carrying out feature scaling. While scaling to zero mean, unit variance is generally prepared, we found that scale all values between the minimum and maximum values in the data gave slightly improved performance. Scaling is used to ensure that particular features are not given excessive weighting such that the importance of other features is diminished a priori to building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#new feature scaling to keep dataframe format\n",
    "ss = {}\n",
    "do_scale = True\n",
    "        \n",
    "if do_scale:\n",
    "    for col in features_to_process:\n",
    "        data = df_train[col].append(df_val[col]).append(df_test[col])\n",
    "\n",
    "        ss[col] = preprocessing.MinMaxScaler() #define and store a label encoder for every column so we can inverse_transform  \n",
    "        ss[col].fit(data.values)\n",
    "        \n",
    "        if col in df_train.columns.values:\n",
    "            df_train[col] = ss[col].transform(df_train[col])\n",
    "            df_val[col] = ss[col].transform(df_val[col])\n",
    "            \n",
    "        if col in df_test.columns.values:\n",
    "            df_test[col] = ss[col].transform(df_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the data again in order to avoid repeating all this time intensive processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the data\n",
    "data_path_out = 'data/'\n",
    "df_train.to_csv(data_path_out + \"train_preprocessed_model.csv\", encoding=\"utf-8\", index=False)\n",
    "df_val.to_csv(data_path_out + \"validation_preprocessed_model.csv\", encoding=\"utf-8\", index=False)\n",
    "df_test.to_csv(data_path_out + \"test_preprocessed_model.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation and Preparation\n",
    "Let's start by re-importing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we import the cleaned and pre-processed data\n",
    "data_path_in = 'data/'\n",
    "df_train = pd.read_csv(data_path_in + \"train_preprocessed_model.csv\")\n",
    "df_val = pd.read_csv(data_path_in + \"validation_preprocessed_model.csv\")\n",
    "df_test = pd.read_csv(data_path_in + \"test_preprocessed_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Development Data\n",
    "We extract data from the training data in the exact format of the validation data as our development data which is what we will use to optimize our model on (this leaves the validation data for validation of the results we would expect on the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation data contains 299523 non-clicks\n",
      "The validation data contains 226 clicks\n",
      "Total: 299749 data points\n"
     ]
    }
   ],
   "source": [
    "print (\"The validation data contains {} non-clicks\".format(len(df_val[df_val.click == 0])))\n",
    "print (\"The validation data contains {} clicks\".format(len(df_val[df_val.click == 1])))\n",
    "print (\"Total: {} data points\".format(len(df_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_point = len(df_train)-299749\n",
    "df_dev = df_train[cut_point:].copy()\n",
    "df_train = df_train[:cut_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development data contains 299519 non-clicks\n",
      "The development data contains 230 clicks\n",
      "Total: 299749 data points\n"
     ]
    }
   ],
   "source": [
    "print (\"The development data contains {} non-clicks\".format(len(df_dev[df_dev.click == 0])))\n",
    "print (\"The development data contains {} clicks\".format(len(df_dev[df_dev.click == 1])))\n",
    "print (\"Total: {} data points\".format(len(df_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Undersampling\n",
    "We select all the clicks in the training data and 15,000 non-clicks on which to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_process = df_test.columns.values\n",
    "features_remove = ['click', 'bidid', 'logtype', 'userid', 'urlid', 'bidprice', 'payprice', 'usertag']\n",
    "features = [x for x in features_to_process if x not in features_remove]\n",
    "classification_column = 'click'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "num_to_sample = 15000\n",
    "\n",
    "#Full datasets\n",
    "# X_train_full = df_train[features]\n",
    "X_val_full = df_val[features]\n",
    "X_dev_full = df_dev[features]\n",
    "X_test = df_test[features]\n",
    "\n",
    "\n",
    "#Training\n",
    "df_train_sample = df_train.sample(num_to_sample, random_state=rand_seed)\n",
    "X_train_inliers = df_train_sample[features][df_train.click == 0]\n",
    "y_train_inliers = df_train_sample[classification_column][df_train.click == 0]\n",
    "\n",
    "X_train_outliers = df_train[features][df_train.click == 1]\n",
    "y_train_outliers = df_train[classification_column][df_train.click == 1]\n",
    "\n",
    "X_train = X_train_inliers\n",
    "y_train = y_train_inliers\n",
    "\n",
    "#Validation\n",
    "X_val_inliers = df_val[features][df_val.click == 0]\n",
    "y_val_inliers = df_val[classification_column][df_val.click == 0]\n",
    "\n",
    "X_val_outliers = df_val[features][df_val.click == 1]\n",
    "y_val_outliers = df_val[classification_column][df_val.click == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection (Recursive Feature Elimination)\n",
    "We perform Recursive Feature Elimination in order to obtain the set of features which provide the best score on the validation set. Common scoring functions include precision, recall, F-score or Area Under the Curve (AUC), however, essential for our model is the proportion of clicks it can properly correct (True Positives) against the proportion of non-clicks it believes are outliers/clicks (False Positives). Since True and False Negatives have very little impact on our bidding strategy as we are simply ignoring all negative predictions, we use this ratio as our scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiate the variables for Recursive Feature Elimination\n",
    "\n",
    "do_remnant = False #This is a flag for remnant analysis (described below)\n",
    "\n",
    "list_features = features.copy()\n",
    "list_features_remove = []\n",
    "num_i = len(list_features)\n",
    "\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_val_click = np.array(list(df_val['click']))\n",
    "\n",
    "if do_remnant:\n",
    "    true_val_remnants = np.array(list(df_val['click_remnants']))\n",
    "\n",
    "cur_i = 0\n",
    "def da_ocsvm_model(features=[]):\n",
    "    global results_dict\n",
    "    global cur_i\n",
    "    \n",
    "    results_dict[cur_i] = {}\n",
    "    results_dict[cur_i]['features'] = features\n",
    "    \n",
    "    start_time = time.time()\n",
    "    one_class_svm = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "    one_class_svm.fit(X_train[features])\n",
    "    \n",
    "    if not do_remnant:\n",
    "        y_pred_val_inliers = one_class_svm.predict(X_val_inliers[features])\n",
    "        y_pred_val_outliers = one_class_svm.predict(X_val_outliers[features])\n",
    "    else:\n",
    "        y_pred_val_full = one_class_svm.predict(X_val_full[features])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print (\"Time taken: {:.2f}s\".format(end_time - start_time))    \n",
    "    \n",
    "    if not do_remnant:\n",
    "        num_outliers_as_outliers = len(np.extract(y_pred_val_outliers == -1, y_pred_val_outliers))\n",
    "        perc_clean_as_outliers = len(np.extract(y_pred_val_inliers == -1, y_pred_val_inliers))*100/len(y_pred_val_inliers)\n",
    "        perc_outliers_as_outliers = num_outliers_as_outliers*100/len(y_pred_val_outliers)\n",
    "    else:\n",
    "        #REMNANT VALIDATION - START\n",
    "        # Compare predited outliers (-1) with click remnants (1) \n",
    "        num_clean_as_outliers = np.sum(np.logical_and(y_pred_val_full == -1, true_val_click == 0))\n",
    "        num_outliers_as_outliers = np.sum(np.logical_and(y_pred_val_full == -1, true_val_remnants == 1))\n",
    "        \n",
    "        perc_clean_as_outliers = num_clean_as_outliers*100/(true_val_click == 0).sum()\n",
    "        perc_outliers_as_outliers = num_outliers_as_outliers*100/(true_val_click == 1).sum()\n",
    "    \n",
    "    score_ratio = perc_outliers_as_outliers/perc_clean_as_outliers\n",
    "    \n",
    "    results_dict[cur_i]['num_outliers_as_outliers'] = num_outliers_as_outliers\n",
    "    results_dict[cur_i]['perc_clean_as_outliers'] = perc_clean_as_outliers\n",
    "    results_dict[cur_i]['perc_outliers_as_outliers'] = perc_outliers_as_outliers\n",
    "    results_dict[cur_i]['score_ratio'] = score_ratio\n",
    "    \n",
    "    cur_i += 1\n",
    "    \n",
    "    print (\"Score: {:.4f}  |  % Out as out: {:.4f}  |  Num Outliers Detected: {}  |  Features: {}\" \\\n",
    "           .format(score_ratio, perc_outliers_as_outliers, num_outliers_as_outliers, str(features)))\n",
    "    print ()\n",
    "    \n",
    "    return score_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print out the final values to allow flexibility over which features to choose. Some feature combinations give better scores but predict less clicks, which will cost our strategy less but less clicks will be found in total, while others are capable of detecting more clicks in the data at a greater score penalty, meaning we will be paying more per click (higher CPC) but will be capable of detecting more of them, at a slightly increased cost.\n",
    "\n",
    "The *do_remnant* part of the code above was an attempt to find a different set of features to the best model, capable and optimized on finding clicks which the best model could not detect. However, only 13 additional clicks could be detected by our most optimized model and thus, this approach was discarded over use of alternative model structures to a One-Class SVM.\n",
    "\n",
    "Below is a summarised example of RFE results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Iteration: 77\n",
    "Worst score: 4.312485453417858  |  Best score: 6.402526612799795  |  Removed feature: usertag_16706  |  Time: 14.09s\n",
    "Score: 6.4025  |  % Out as out: 58.8496  |  Num Outliers Detected: 133\n",
    "Features: ['slotwidth', 'slotformat', 'os', 'slotprice_0', 'slotprice_1_10', 'slotprice_50_100', 'slotprice_50', 'usertag_10006', 'usertag_10116', 'usertag_10149', 'usertag_10684', 'usertag_11278', 'usertag_11680', 'usertag_13042', 'usertag_16617', 'usertag_16753']\n",
    "\n",
    "Iteration: 78\n",
    "Worst score: 4.150569656777096  |  Best score: 6.563790195236241  |  Removed feature: slotwidth  |  Time: 13.83s\n",
    "Score: 6.5638  |  % Out as out: 61.9469  |  Num Outliers Detected: 140\n",
    "Features: ['slotformat', 'os', 'slotprice_0', 'slotprice_1_10', 'slotprice_50_100', 'slotprice_50', 'usertag_10006', 'usertag_10116', 'usertag_10149', 'usertag_10684', 'usertag_11278', 'usertag_11680', 'usertag_13042', 'usertag_16617', 'usertag_16753']\n",
    "\n",
    "Iteration: 79\n",
    "Worst score: 4.142436312339226  |  Best score: 6.74298593046714  |  Removed feature: usertag_10149  |  Time: 13.50s\n",
    "Score: 6.7430  |  % Out as out: 54.8673  |  Num Outliers Detected: 124\n",
    "Features: ['slotformat', 'os', 'slotprice_0', 'slotprice_1_10', 'slotprice_50_100', 'slotprice_50', 'usertag_10006', 'usertag_10116', 'usertag_10684', 'usertag_11278', 'usertag_11680', 'usertag_13042', 'usertag_16617', 'usertag_16753']\n",
    "\n",
    "Iteration: 80\n",
    "Worst score: 4.360611279859623  |  Best score: 6.840161551212469  |  Removed feature: usertag_16753  |  Time: 13.03s\n",
    "Score: 6.8402  |  % Out as out: 54.4248  |  Num Outliers Detected: 123\n",
    "Features: ['slotformat', 'os', 'slotprice_0', 'slotprice_1_10', 'slotprice_50_100', 'slotprice_50', 'usertag_10006', 'usertag_10116', 'usertag_10684', 'usertag_11278', 'usertag_11680', 'usertag_13042', 'usertag_16617']\n",
    "\n",
    "Iteration: 81\n",
    "Worst score: 3.571894687980644  |  Best score: 6.4237720871869985  |  Removed feature: slotformat  |  Time: 12.59s\n",
    "Score: 6.4238  |  % Out as out: 57.5221  |  Num Outliers Detected: 130\n",
    "Features: ['os', 'slotprice_0', 'slotprice_1_10', 'slotprice_50_100', 'slotprice_50', 'usertag_10006', 'usertag_10116', 'usertag_10684', 'usertag_11278', 'usertag_11680', 'usertag_13042', 'usertag_16617']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the peak in number of *clicks* detected occurs earlier than the peak in best score, however, we opt for the feature set which managed to identify the most *clicks* even at the cost of a slightly worse score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Hyper-parameters\n",
    "We also attempt to optimize hyper-parameters in order to further fine-tune our model through a custom grid-search implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the data to the model\n",
    "def build_model(nu=0.1, gamma=0.1):\n",
    "    start_time = time.time()\n",
    "    one_class_svm = OneClassSVM(nu=nu, kernel=\"rbf\", gamma=gamma) # tol=0.0001\n",
    "    one_class_svm.fit(X_train[features])\n",
    "\n",
    "    y_pred_val_inliers = one_class_svm.predict(X_val_inliers[features])\n",
    "    y_pred_val_outliers = one_class_svm.predict(X_val_outliers[features])\n",
    "\n",
    "    y_pred_test = one_class_svm.predict(X_test[features])\n",
    "\n",
    "    print (\"Time taken: {:.2f}s\".format(time.time() - start_time))\n",
    "    \n",
    "    num_outliers_as_outliers = len(np.extract(y_pred_val_outliers == -1, y_pred_val_outliers))\n",
    "    perc_clean_as_outliers = len(np.extract(y_pred_val_inliers == -1, y_pred_val_inliers))*100/len(y_pred_val_inliers)\n",
    "    perc_outliers_as_outliers = num_outliers_as_outliers*100/len(y_pred_val_outliers)\n",
    "    score_ratio = perc_outliers_as_outliers/perc_clean_as_outliers\n",
    "    params = str(nu) + \", \" + str(gamma)\n",
    "    \n",
    "    print (\"Score: {:.4f}  |  % Clean as out: {:.4f}  |  % Out as out: {:.4f}  |  Num Outliers Detected: {}  |  Params: {}\" \\\n",
    "           .format(score_ratio, perc_clean_as_outliers, perc_outliers_as_outliers, num_outliers_as_outliers, params))\n",
    "    print ()\n",
    "    \n",
    "    return score_ratio, num_outliers_as_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameter optimization (grid-search)\n",
    "dict_results = {}\n",
    "incr = 0.02\n",
    "list_nu = np.arange(incr,1 + incr,incr)\n",
    "incr = 0.01\n",
    "list_gamma = np.arange(incr,1 + incr,incr)\n",
    "for nu in list_nu:\n",
    "    dict_results[nu] = {}\n",
    "    for gamma in list_gamma:\n",
    "        dict_results[nu][gamma] = {}\n",
    "        dict_results[nu][gamma]['score_ratio'], dict_results[nu][gamma]['num_outliers_as_outliers'] = build_model(nu, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that while it was possible to optimize and improve the number of clicks detected, this came at tremendous cost our our scoring function, essentially severely limiting our target bidding strategy. This follows from the fact that the current best feature combination was selected to provide the best separation of the vector space for the hyper-parameters the model was tuned on. A point for further improvement could involve running various Recursive Feature Elimination runs on different models at different hyper-parameters and then selecting the best combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our best features from the Recursive Feature Elimination\n",
    "features = ['slotformat', 'os', 'slotprice_0', 'slotprice_1_10', 'slotprice_50_100', 'slotprice_50', 'usertag_10006', \\\n",
    "            'usertag_10116', 'usertag_10149', 'usertag_10684', 'usertag_11278', 'usertag_11680', 'usertag_13042', \\\n",
    "            'usertag_16617', 'usertag_16753']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 26.52s\n",
      "Score: 6.4939  |  % Clean as out: 9.4030  |  % Out as out: 61.0619  |  Num Outliers Detected: 138  |  Params: 0.1, 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "nu = 0.1\n",
    "gamma = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "one_class_svm = OneClassSVM(nu=nu, kernel=\"rbf\", gamma=gamma) # tol=0.0001\n",
    "one_class_svm.fit(X_train[features])\n",
    "\n",
    "#Make the predictions\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred_val_inliers = one_class_svm.predict(X_val_inliers[features])\n",
    "y_pred_val_outliers = one_class_svm.predict(X_val_outliers[features])\n",
    "\n",
    "y_pred_test = one_class_svm.predict(X_test[features])\n",
    "\n",
    "print (\"Time taken: {:.2f}s\".format(time.time() - start_time))\n",
    "\n",
    "num_outliers_as_outliers = len(np.extract(y_pred_val_outliers == -1, y_pred_val_outliers))\n",
    "perc_clean_as_outliers = len(np.extract(y_pred_val_inliers == -1, y_pred_val_inliers))*100/len(y_pred_val_inliers)\n",
    "perc_outliers_as_outliers = num_outliers_as_outliers*100/len(y_pred_val_outliers)\n",
    "score_ratio = perc_outliers_as_outliers/perc_clean_as_outliers\n",
    "params = str(nu) + \", \" + str(gamma)\n",
    "\n",
    "print (\"Score: {:.4f}  |  % Clean as out: {:.4f}  |  % Out as out: {:.4f}  |  Num Outliers Detected: {}  |  Params: {}\" \\\n",
    "       .format(score_ratio, perc_clean_as_outliers, perc_outliers_as_outliers, num_outliers_as_outliers, params))\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "Since during RFE, we based our optimization on performance on the validation set, let's now validate those results on the *dev* set to ensure we still get a high amount of clicks on a completely isolated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "# Obtain the predictions for the full validation dataset\n",
    "y_pred_val = one_class_svm.predict(X_val_full[features])\n",
    "\n",
    "# Obtain predictions for the dev dataset\n",
    "y_pred_dev = one_class_svm.predict(df_dev[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 5.1840  |  % Clean as out: 9.6451  |  % Out as out: 50.0000  |  Num Outliers Detected: 115  |  Params: 0.1, 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's print the actual number of clicks correctly predicted and those incorrectly predicted\n",
    "true_dev_click = np.array(list(df_dev['click'])) # These are the true click values\n",
    "\n",
    "num_clean_as_outliers = np.sum(np.logical_and(y_pred_dev == -1, true_dev_click == 0))\n",
    "num_outliers_as_outliers = np.sum(np.logical_and(y_pred_dev == -1, true_dev_click == 1))\n",
    "\n",
    "perc_clean_as_outliers = num_clean_as_outliers*100/(true_dev_click == 0).sum()\n",
    "perc_outliers_as_outliers = num_outliers_as_outliers*100/(true_dev_click == 1).sum()\n",
    "\n",
    "score_ratio = perc_outliers_as_outliers/perc_clean_as_outliers\n",
    "\n",
    "print (\"Score: {:.4f}  |  % Clean as out: {:.4f}  |  % Out as out: {:.4f}  |  Num Outliers Detected: {}  |  Params: {}\" \\\n",
    "       .format(score_ratio, perc_clean_as_outliers, perc_outliers_as_outliers, num_outliers_as_outliers, params))\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the Predictions\n",
    "Finally, we export out predictions for analysis in our bidding strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the predictions to our class values (so -1s should be 1s and 1s should be 0s)\n",
    "processed = False\n",
    "if not processed:\n",
    "    y_pred_val[y_pred_val == 1] = 0\n",
    "    y_pred_dev[y_pred_dev == 1] = 0\n",
    "    y_pred_test[y_pred_test == 1] = 0\n",
    "\n",
    "    y_pred_val[y_pred_val == -1] = 1\n",
    "    y_pred_dev[y_pred_dev == -1] = 1\n",
    "    y_pred_test[y_pred_test == -1] = 1\n",
    "\n",
    "    processed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the predictions to the dataframes\n",
    "df_val['1CSVM_pred'] = y_pred_val\n",
    "df_dev['1CSVM_pred'] = y_pred_dev\n",
    "df_test['1CSVM_pred'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "data_path_out = 'data/'\n",
    "features_to_save = ['bidid', '1CSVM_pred']\n",
    "df_val[features_to_save].to_csv(data_path_out + \"submission_val_ocsvm.csv\", encoding=\"utf-8\", index=False)\n",
    "df_dev[features_to_save].to_csv(data_path_out + \"submission_dev_ocsvm.csv\", encoding=\"utf-8\", index=False)\n",
    "df_test[features_to_save].to_csv(data_path_out + \"submission_test_ocsvm.csv\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "# We also store the dev dataset\n",
    "df_dev.to_csv(data_path_out + \"dev.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "We will be validating our model on the *dev* dataset as that is completely unseen by our current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/dev.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-72a9b094bd7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Importing the dataset to validate on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/dev.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/AlexAir/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AlexAir/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AlexAir/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AlexAir/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AlexAir/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/dev.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Importing the dataset to validate on\n",
    "df_val = pd.read_csv(\"data/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clicks in the data set we are validating against is 230\n"
     ]
    }
   ],
   "source": [
    "# Total clicks in the validation data set\n",
    "print (\"Total clicks in the data set we are validating against is {}\".format(df_val['click'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the dataset containing our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_val_submit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7b6be9f9a837>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcol_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'bidid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bidprice'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# df_val_submit = pd.read_csv(\"data/submission_dev_ocsvm.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_val_submit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_val_submit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'click_predict'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_val_submit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bidprice'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_val_submit' is not defined"
     ]
    }
   ],
   "source": [
    "col_names = ['bidid', 'bidprice']\n",
    "df_val_submit = pd.read_csv(\"data/submission_dev_ocsvm.csv\")\n",
    "df_val_submit.columns = col_names\n",
    "df_val_submit['click_predict'] = df_val_submit['bidprice'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a basic bidding strategy using a constant bid for every item our model has predicted as a click."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bidid</th>\n",
       "      <th>bidprice</th>\n",
       "      <th>click_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a357d4941f6bdc4a5d76c63a0b576f56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c72018ffc849cf1e54dd09e18aedfb8f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7065c61e0fb88cd4b9f9d48cc0a20f9c</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13f3fbf80f40d9c031868e14213b3b38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ffe1c06a53a46b2aeb469a8a5021b7a9</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3d7e75c0f6ce7bc4fe32e3c5f42c4018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               bidid  bidprice  click_predict\n",
       "5   a357d4941f6bdc4a5d76c63a0b576f56       0.0            0.0\n",
       "6   c72018ffc849cf1e54dd09e18aedfb8f       0.0            0.0\n",
       "7   7065c61e0fb88cd4b9f9d48cc0a20f9c     300.0            1.0\n",
       "8   13f3fbf80f40d9c031868e14213b3b38       0.0            0.0\n",
       "9   ffe1c06a53a46b2aeb469a8a5021b7a9     300.0            1.0\n",
       "10  3d7e75c0f6ce7bc4fe32e3c5f42c4018       0.0            0.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 300\n",
    "df_val_submit['bidprice'] = df_val_submit['click_predict']*c\n",
    "df_val_submit[5:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Function\n",
    "Now, we run our validation function and extract important metrics for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glob_cash_in_bank = 25000000\n",
    "glob_cash_in_bank = glob_cash_in_bank * (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_results(df, cash_in_bank, random=True):\n",
    "    col_name_validate = 'bidprice_validate'\n",
    "    \n",
    "    df_temp = df[df[col_name_validate] > 0]\n",
    "    if random == True:\n",
    "        df_temp = df_temp.sample(frac=1, random_state=rand_seed).reset_index(drop=True)\n",
    "    \n",
    "    strategy_impressions = 0\n",
    "    strategy_clicks = 0\n",
    "    n_rows_in_budget = 0\n",
    "    \n",
    "    for row in df_temp.iterrows():\n",
    "        row = row[1]\n",
    "        if cash_in_bank > 0:\n",
    "            n_rows_in_budget += 1\n",
    "            if row[col_name_validate] > row['payprice']: #was bidprice\n",
    "                strategy_impressions += 1\n",
    "                strategy_clicks += int(row['click'])\n",
    "                cash_in_bank -= row['payprice'] #was bidprice but Jun Wang said payprice\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return cash_in_bank, strategy_impressions, strategy_clicks\n",
    "\n",
    "def calc_results(df_validate, df_submit, budget_ratio):\n",
    "    glob_cash_in_bank = 25000000\n",
    "    glob_cash_in_bank = glob_cash_in_bank * budget_ratio\n",
    "    \n",
    "    cash_in_bank = glob_cash_in_bank\n",
    "    df_validate['bidprice_validate'] = df_submit['bidprice'].copy()\n",
    "\n",
    "    cash_in_bank, strategy_impressions, strategy_clicks = \\\n",
    "                    validate_results(df=df_validate, cash_in_bank=cash_in_bank, random=True)\n",
    "    cost = (glob_cash_in_bank-cash_in_bank)/1000\n",
    "    ctr = strategy_clicks/strategy_impressions\n",
    "    cpc = cost/strategy_clicks\n",
    "    \n",
    "    return cost, strategy_impressions, strategy_clicks, ctr, cpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our validation function for a range of different budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_strategy_results(df_validate, df_submit):\n",
    "    budget_ratios = [1, 1/2, 1/4, 1/8, 1/16]\n",
    "    budget_ratio_names = ['Full', '1/2', '1/4', '1/8', '1/16']\n",
    "    for i, budget_ratio in enumerate(budget_ratios):\n",
    "        cost, strategy_impressions, strategy_clicks, ctr, cpc = calc_results(df_validate, df_submit, budget_ratio)\n",
    "        print (\"{} Budget:\".format(budget_ratio_names[i]))\n",
    "        print (\"Cost: ${:.2f}  |  Impressions: {:.0f}   |   Clicks: {:.0f}  |  CTR: {:.5f}%  |  CPC: ${:.2f}\" \\\n",
    "           .format(cost, strategy_impressions, strategy_clicks, ctr*100, cpc))\n",
    "        print ()\n",
    "        \n",
    "# print_strategy_results(df_val, df_val_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare this very basic bidding strategy on our improved model to the results from the preliminary submission on the test set shown below:\n",
    "```\n",
    "Full Budget:\n",
    "Cost: $7756.04  |  Impressions: 89606   |   Clicks: 109  |  CTR: 0.00122%  |  CPC: $71.16\n",
    "\n",
    "1/2 Budget:\n",
    "Cost: $7756.04  |  Impressions: 89606   |   Clicks: 109  |  CTR: 0.00122%  |  CPC: $71.16\n",
    "\n",
    "1/4 Budget:\n",
    "Cost: $6249.99  |  Impressions: 72154   |   Clicks: 87  |  CTR: 0.00121%  |  CPC: $71.84\n",
    "\n",
    "1/8 Budget:\n",
    "Cost: $3124.99  |  Impressions: 35961   |   Clicks: 42  |  CTR: 0.00117%  |  CPC: $74.40\n",
    "\n",
    "1/16 Budget:\n",
    "Cost: $1562.00  |  Impressions: 17918   |   Clicks: 20  |  CTR: 0.00111%  |  CPC: $78.10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that we have significantly improved number of clicks, CTR and CPC. And have a significant amount of budget left to spend. Therefore, we combine this model with others to try and locate the remaining clicks in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Bidding Strategy\n",
    "#### One-Class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by analyzing how our OCSVM model performs on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the dataset to validate on\n",
    "df_val = pd.read_csv(\"data/validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total clicks in the data set we are validating against is 226\n"
     ]
    }
   ],
   "source": [
    "# Total clicks in the validation data set\n",
    "print (\"Total clicks in the data set we are validating against is {}\".format(df_val['click'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import and create a strategy for our OCSVM predictions on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Budget:\n",
      "Cost: $2645.05  |  Impressions: 28291   |   Clicks: 138  |  CTR: 0.48779%  |  CPC: $19.17\n",
      "\n",
      "1/2 Budget:\n",
      "Cost: $2645.05  |  Impressions: 28291   |   Clicks: 138  |  CTR: 0.48779%  |  CPC: $19.17\n",
      "\n",
      "1/4 Budget:\n",
      "Cost: $2645.05  |  Impressions: 28291   |   Clicks: 138  |  CTR: 0.48779%  |  CPC: $19.17\n",
      "\n",
      "1/8 Budget:\n",
      "Cost: $2645.05  |  Impressions: 28291   |   Clicks: 138  |  CTR: 0.48779%  |  CPC: $19.17\n",
      "\n",
      "1/16 Budget:\n",
      "Cost: $1562.52  |  Impressions: 16682   |   Clicks: 78  |  CTR: 0.46757%  |  CPC: $20.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['bidid', 'bidprice']\n",
    "df_submit_ocsvm = pd.read_csv(\"data/submission_val_ocsvm.csv\")\n",
    "df_submit_ocsvm.columns = col_names\n",
    "df_submit_ocsvm['click_predict'] = df_submit_ocsvm['bidprice'].copy()\n",
    "\n",
    "c = 300\n",
    "df_submit_ocsvm['bidprice'] = df_submit_ocsvm['click_predict']*c\n",
    "\n",
    "print_strategy_results(df_val, df_submit_ocsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the same for Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submit_lr = pd.read_csv(\"data/lr_validation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submit_lr = df_submit_lr[['bidid', 'clickpred']].copy()\n",
    "df_submit_lr.columns = col_names\n",
    "df_submit_lr['click_predict'] = df_submit_lr['bidprice'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a basic constant bidding strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bidid</th>\n",
       "      <th>bidprice</th>\n",
       "      <th>click_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>903211f027ca70f611544f42b94db094</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15274113eac79b065e186c1962c33632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9c963e762671efbc15c4e9fb3cef9c46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a483f16043e9f0057d542ee724cd81cb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2da6d0891a81ac8233bba9bcffde2159</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b887b88eccaf5ba1addf3171e91de883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               bidid  bidprice  click_predict\n",
       "5   903211f027ca70f611544f42b94db094       300              1\n",
       "6   15274113eac79b065e186c1962c33632         0              0\n",
       "7   9c963e762671efbc15c4e9fb3cef9c46         0              0\n",
       "8   a483f16043e9f0057d542ee724cd81cb         0              0\n",
       "9   2da6d0891a81ac8233bba9bcffde2159       300              1\n",
       "10  b887b88eccaf5ba1addf3171e91de883         0              0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 300\n",
    "df_submit_lr['bidprice'] = df_submit_lr['click_predict']*c\n",
    "df_submit_lr[5:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Budget:\n",
      "Cost: $3943.25  |  Impressions: 44173   |   Clicks: 153  |  CTR: 0.34637%  |  CPC: $25.77\n",
      "\n",
      "1/2 Budget:\n",
      "Cost: $3943.25  |  Impressions: 44173   |   Clicks: 153  |  CTR: 0.34637%  |  CPC: $25.77\n",
      "\n",
      "1/4 Budget:\n",
      "Cost: $3943.25  |  Impressions: 44173   |   Clicks: 153  |  CTR: 0.34637%  |  CPC: $25.77\n",
      "\n",
      "1/8 Budget:\n",
      "Cost: $3125.02  |  Impressions: 35046   |   Clicks: 122  |  CTR: 0.34811%  |  CPC: $25.61\n",
      "\n",
      "1/16 Budget:\n",
      "Cost: $1562.62  |  Impressions: 17434   |   Clicks: 57  |  CTR: 0.32695%  |  CPC: $27.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_strategy_results(df_val, df_submit_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "We do the same for our Neural Network driven model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_submit_nn = pd.read_csv(\"data/nn_val_preds.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_names = ['bidid', 'bidprice']\n",
    "df_submit_nn = df_submit_nn[['bidid', 'clickprob']].copy()\n",
    "df_submit_nn.columns = col_names\n",
    "df_submit_nn['click_predict'] = df_submit_nn['bidprice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a basic constant bidding strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bidid</th>\n",
       "      <th>bidprice</th>\n",
       "      <th>click_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91c6a6b9e90c0f54d3230815a5a3e22e</td>\n",
       "      <td>8.400143</td>\n",
       "      <td>0.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24b3621ad3b063b6c09c541781d534b3</td>\n",
       "      <td>521.444527</td>\n",
       "      <td>0.521445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              bidid    bidprice  click_predict\n",
       "0  91c6a6b9e90c0f54d3230815a5a3e22e    8.400143       0.008400\n",
       "1  24b3621ad3b063b6c09c541781d534b3  521.444527       0.521445"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 1000\n",
    "df_submit_nn['bidprice'] = df_submit_nn['click_predict']*c\n",
    "df_submit_nn[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Budget:\n",
      "Cost: $4914.40  |  Impressions: 101721   |   Clicks: 160  |  CTR: 0.15729%  |  CPC: $30.71\n",
      "\n",
      "1/2 Budget:\n",
      "Cost: $4914.40  |  Impressions: 101721   |   Clicks: 160  |  CTR: 0.15729%  |  CPC: $30.71\n",
      "\n",
      "1/4 Budget:\n",
      "Cost: $4914.40  |  Impressions: 101721   |   Clicks: 160  |  CTR: 0.15729%  |  CPC: $30.71\n",
      "\n",
      "1/8 Budget:\n",
      "Cost: $3125.08  |  Impressions: 64696   |   Clicks: 99  |  CTR: 0.15302%  |  CPC: $31.57\n",
      "\n",
      "1/16 Budget:\n",
      "Cost: $1562.55  |  Impressions: 32148   |   Clicks: 55  |  CTR: 0.17108%  |  CPC: $28.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_strategy_results(df_val, df_submit_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_submit_nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-755b21bc7f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_submit_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_submit_nn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bidid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clickpred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_submit_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_submit_nn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'click_predict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_submit_nn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bidprice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_submit_nn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bidprice'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_submit_nn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'click_predict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_submit_nn' is not defined"
     ]
    }
   ],
   "source": [
    "df_submit_nn = df_submit_nn[['bidid', 'clickpred']].copy()\n",
    "df_submit_nn.columns = col_names\n",
    "df_submit_nn['click_predict'] = df_submit_nn['bidprice'].copy()\n",
    "c = 300\n",
    "df_submit_nn['bidprice'] = df_submit_nn['click_predict']*c\n",
    "df_submit_nn[0:2]\n",
    "print_strategy_results(df_val, df_submit_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Model\n",
    "We note that performance for our Logistic Regression model is also very good, therefore, we attempt a combined model which ensembles both model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submit_combined = df_submit_ocsvm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submit_combined['click_predict_lr'] = df_submit_lr['click_predict'].copy()\n",
    "df_submit_combined['click_predict_nn'] = df_submit_nn['click_predict'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a basic constant bidding strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bidid</th>\n",
       "      <th>bidprice</th>\n",
       "      <th>click_predict</th>\n",
       "      <th>click_predict_lr</th>\n",
       "      <th>click_predict_nn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>903211f027ca70f611544f42b94db094</td>\n",
       "      <td>196.602668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15274113eac79b065e186c1962c33632</td>\n",
       "      <td>4.867544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9c963e762671efbc15c4e9fb3cef9c46</td>\n",
       "      <td>45.089511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a483f16043e9f0057d542ee724cd81cb</td>\n",
       "      <td>18.251640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2da6d0891a81ac8233bba9bcffde2159</td>\n",
       "      <td>194.874308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b887b88eccaf5ba1addf3171e91de883</td>\n",
       "      <td>83.570907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               bidid    bidprice  click_predict  \\\n",
       "5   903211f027ca70f611544f42b94db094  196.602668            0.0   \n",
       "6   15274113eac79b065e186c1962c33632    4.867544            0.0   \n",
       "7   9c963e762671efbc15c4e9fb3cef9c46   45.089511            0.0   \n",
       "8   a483f16043e9f0057d542ee724cd81cb   18.251640            0.0   \n",
       "9   2da6d0891a81ac8233bba9bcffde2159  194.874308            0.0   \n",
       "10  b887b88eccaf5ba1addf3171e91de883   83.570907            0.0   \n",
       "\n",
       "    click_predict_lr  click_predict_nn  \n",
       "5                  1          0.055375  \n",
       "6                  0          0.013750  \n",
       "7                  0          0.127371  \n",
       "8                  0          0.051558  \n",
       "9                  1          0.050492  \n",
       "10                 0          0.236076  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 177\n",
    "w_ocsvm = 1 # Weighting for the One-Class SVM Model\n",
    "w_lr = 1 # Weighting for the Logistic Regression Model\n",
    "w_nn = 2 # Weighting for the Neural Network Model\n",
    "\n",
    "df_submit_combined['bidprice'] = (w_ocsvm*df_submit_combined['click_predict'] + w_lr*df_submit_combined['click_predict_lr'] \\\n",
    "                                 + w_nn*df_submit_combined['click_predict_nn'])*c\n",
    "df_submit_combined[5:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Budget:\n",
      "Cost: $4800.04  |  Impressions: 75968   |   Clicks: 169  |  CTR: 0.22246%  |  CPC: $28.40\n",
      "\n",
      "1/2 Budget:\n",
      "Cost: $4800.04  |  Impressions: 75968   |   Clicks: 169  |  CTR: 0.22246%  |  CPC: $28.40\n",
      "\n",
      "1/4 Budget:\n",
      "Cost: $4800.04  |  Impressions: 75968   |   Clicks: 169  |  CTR: 0.22246%  |  CPC: $28.40\n",
      "\n",
      "1/8 Budget:\n",
      "Cost: $3125.03  |  Impressions: 49420   |   Clicks: 106  |  CTR: 0.21449%  |  CPC: $29.48\n",
      "\n",
      "1/16 Budget:\n",
      "Cost: $1562.56  |  Impressions: 24518   |   Clicks: 57  |  CTR: 0.23248%  |  CPC: $27.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_strategy_results(df_val, df_submit_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Budget:\n",
      "Cost: $5113.05  |  Impressions: 83167   |   Clicks: 172  |  CTR: 0.20681%  |  CPC: $29.73\n",
      "\n",
      "1/2 Budget:\n",
      "Cost: $5113.05  |  Impressions: 83167   |   Clicks: 172  |  CTR: 0.20681%  |  CPC: $29.73\n",
      "\n",
      "1/4 Budget:\n",
      "Cost: $5113.05  |  Impressions: 83167   |   Clicks: 172  |  CTR: 0.20681%  |  CPC: $29.73\n",
      "\n",
      "1/8 Budget:\n",
      "Cost: $3125.05  |  Impressions: 50694   |   Clicks: 102  |  CTR: 0.20121%  |  CPC: $30.64\n",
      "\n",
      "1/16 Budget:\n",
      "Cost: $1562.53  |  Impressions: 25206   |   Clicks: 58  |  CTR: 0.23010%  |  CPC: $26.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 177\n",
    "w_ocsvm = 1 # Weighting for the One-Class SVM Model\n",
    "w_lr = 1 # Weighting for the Logistic Regression Model\n",
    "w_nn = 2.5 # Weighting for the Neural Network Model\n",
    "\n",
    "df_submit_combined['bidprice'] = (w_ocsvm*df_submit_combined['click_predict'] + w_lr*df_submit_combined['click_predict_lr'] \\\n",
    "                                 + w_nn*df_submit_combined['click_predict_nn'])*c\n",
    "print_strategy_results(df_val, df_submit_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Budget:\n",
      "Cost: $5438.53  |  Impressions: 90142   |   Clicks: 173  |  CTR: 0.19192%  |  CPC: $31.44\n",
      "\n",
      "1/2 Budget:\n",
      "Cost: $5438.53  |  Impressions: 90142   |   Clicks: 173  |  CTR: 0.19192%  |  CPC: $31.44\n",
      "\n",
      "1/4 Budget:\n",
      "Cost: $5438.53  |  Impressions: 90142   |   Clicks: 173  |  CTR: 0.19192%  |  CPC: $31.44\n",
      "\n",
      "1/8 Budget:\n",
      "Cost: $3125.08  |  Impressions: 51596   |   Clicks: 94  |  CTR: 0.18218%  |  CPC: $33.25\n",
      "\n",
      "1/16 Budget:\n",
      "Cost: $1562.51  |  Impressions: 25705   |   Clicks: 55  |  CTR: 0.21397%  |  CPC: $28.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 177\n",
    "w_ocsvm = 1 # Weighting for the One-Class SVM Model\n",
    "w_lr = 1 # Weighting for the Logistic Regression Model\n",
    "w_nn = 3 # Weighting for the Neural Network Model\n",
    "\n",
    "df_submit_combined['bidprice'] = (w_ocsvm*df_submit_combined['click_predict'] + w_lr*df_submit_combined['click_predict_lr'] \\\n",
    "                                 + w_nn*df_submit_combined['click_predict_nn'])*c\n",
    "print_strategy_results(df_val, df_submit_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Case Implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for Further Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
